version: "3.7"
services:
    minio:
        image: minio/minio:latest
        container_name: "minio"
        networks:
            - druid-net
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 512M
                reservations:
                    cpus: '0.5'
                    memory: 256M
        restart: always
        working_dir: "/minio/storage"
        volumes:
            - ${wd}/minio/storage:/minio/storage
        ports:
            - "9000:9000"
        environment:
            MINIO_ACCESS_KEY: ${S3_ACCESS_KEY}
            MINIO_SECRET_KEY: ${S3_SECRET_KEY}
        command: server /minio/storage
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 20s
            retries: 3

    minio-client:
        image: minio/mc:latest
        container_name: "minio-client"
        networks:
            - druid-net
        depends_on:
            minio:
                condition: service_healthy
        env_file:
            - dockerfiles/druid/druid_environment
        entrypoint: >
            sh -c "mc config host add minio http://minio:9000 ${S3_ACCESS_KEY} ${S3_SECRET_KEY} && mc mb minio/$$S3_STORAGE_BUKCET"

    mlflow:
        container_name: "mlflow"
        networks:
            - druid-net
        build:
            context: dockerfiles/mlflow
        working_dir: "/mlflow"
        volumes:
            - ${wd}/mlflow:/mlflow
        environment:
            MLFLOW_S3_ENDPOINT_URL: http://minio:9000
            AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY}
            AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
        ports:
            - "5000:5000"
        command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri /mlflow/mlruns

    jupyter:
        container_name: "jupyter"
        networks:
            - druid-net
        build:
            context: dockerfiles/jupyter
        volumes:
            - ${wd}/notebooks:/home/jovyan/work
            - ${wd}/aanomaly:/home/jovyan/work/aanomaly
            - ${wd}/mlflow:/mlflow
        environment:
            MLFLOW_S3_ENDPOINT_URL: http://minio:9000
            AWS_ACCESS_KEY_ID: ${S3_ACCESS_KEY}
            AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}    
            MLFLOW_TRACKING_URI: http://mlflow:5000        
        ports:    
            - "8889:8888"

    postgres-dash:
        image: postgres:12.4
        networks:
            - druid-net
        restart: always
        container_name: "postgres-dash"
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 1G
                reservations:
                    cpus: '1'
                    memory: 512M
        ports:
            - "5432:5432"
        environment:
            - POSTGRES_DB=dash
            - POSTGRES_USER=${POSTGRES_DASH_USER_NAME}
            - POSTGRES_PASSWORD=${POSTGRES_DASH_PASSWORD}
        volumes:
            - pg_data_dash:/var/lib/postgresql/data

    dash:
        container_name: "app-dash"
        networks:
            - druid-net
        build:
            context: src
        command: python app.py
        volumes:
            - ./src:/code
        ports:
            - "80:8000"
        depends_on:
            - postgres-dash

    postgres-druid:
        image: postgres:12.4
        networks:
            - druid-net
        restart: always
        container_name: "postgres-druid"
        deploy:
            resources:
                limits:
                    cpus: '1'
                    memory: 1G
                reservations:
                    cpus: '1'
                    memory: 512M
        ports:
            - "5431:5431"
        environment:
            - POSTGRES_DB=druid
            - POSTGRES_USER=${POSTGRES_USER_NAME}
            - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
        volumes:
            - pg_data_druid:/var/lib/postgresql/data

    pgadmin:
        image: dpage/pgadmin4:4.16
        networks:
            - druid-net
        container_name: "pgadmin"
        environment:
            PGADMIN_DEFAULT_EMAIL: "guest"
            PGADMIN_DEFAULT_PASSWORD: "guest"
        volumes:
            - ${wd}/pgadmin/:/var/lib/pgadmin
        ports:
            - "1234:80"
        depends_on:
            - postgres-druid

    zookeeper:
        image: zookeeper:latest
        container_name: "druid-zookeeper"
        networks:
            - druid-net
        ports:
            - 2181:2181
        environment:
            - ZOO_MY_ID=1

    coordinator:
        build: dockerfiles/druid
        image: druid-coordinator:latest
        container_name: "druid-coordinator"
        hostname: coordinator
        networks:
            - druid-net
        depends_on: 
            - postgres-druid
            - zookeeper
            - minio
        ports:
            - "8081:8081"
        env_file:
            - dockerfiles/druid/druid_environment
        environment:
            - DRUID_SERVICE=coordinator
            - DRUID_JVM_ARGS=-server -Xms256m -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
            - S3_ACCESS_KEY=${S3_ACCESS_KEY}
            - S3_SECRET_KEY=${S3_SECRET_KEY}
            - METADATA_STORAGE_USER=${POSTGRES_USER_NAME}
            - METADATA_STORAGE_PASSWORD=${POSTGRES_PASSWORD}

    broker:
        build: dockerfiles/druid
        image: druid-broker:latest
        container_name: "druid-broker"
        hostname: broker
        networks:
            - druid-net
        depends_on: 
            - postgres-druid
            - zookeeper
            - minio
            - coordinator
        ports:
            - "8082:8082"
        mem_limit: "2.2g"
        mem_reservation: 500m
        env_file:
            - dockerfiles/druid/druid_environment
        environment:
            - DRUID_SERVICE=broker
            - DRUID_JVM_ARGS=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -XX:MaxDirectMemorySize=1g -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote.port=17071 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
            - S3_ACCESS_KEY=${S3_ACCESS_KEY}
            - S3_SECRET_KEY=${S3_SECRET_KEY}
            - METADATA_STORAGE_USER=${POSTGRES_USER_NAME}
            - METADATA_STORAGE_PASSWORD=${POSTGRES_PASSWORD}

    historical:
        build: dockerfiles/druid
        image: druid-historical:latest
        container_name: "druid-historical"
        hostname: historical
        networks:
            - druid-net
        ports:
            - "8083:8083"
        depends_on: 
            - postgres-druid
            - zookeeper
            - minio
            - coordinator
        mem_limit: "2.2g"
        mem_reservation: 200m
        env_file:
            - dockerfiles/druid/druid_environment
        environment:
            - DRUID_SERVICE=historical
            - DRUID_JVM_ARGS=-server -Xms1g -Xmx1g -XX:MaxDirectMemorySize=1g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
            - S3_ACCESS_KEY=${S3_ACCESS_KEY}
            - S3_SECRET_KEY=${S3_SECRET_KEY}
            - METADATA_STORAGE_USER=${POSTGRES_USER_NAME}
            - METADATA_STORAGE_PASSWORD=${POSTGRES_PASSWORD}

    overlord:
        build: dockerfiles/druid
        image: druid-overlord:latest
        container_name: "druid-overlord"
        hostname: overlord
        networks:
            - druid-net
        ports:
            - "8090:8090"
        depends_on: 
            - postgres-druid
            - zookeeper
            - minio
            - coordinator
        env_file:
            - dockerfiles/druid/druid_environment
        environment:
            - DRUID_SERVICE=overlord
            - DRUID_JVM_ARGS=-server -Xms256m -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
            - S3_ACCESS_KEY=${S3_ACCESS_KEY}
            - S3_SECRET_KEY=${S3_SECRET_KEY}
            - METADATA_STORAGE_USER=${POSTGRES_USER_NAME}
            - METADATA_STORAGE_PASSWORD=${POSTGRES_PASSWORD}

    middlemanager:
        build: dockerfiles/druid
        image: druid-middlemanager:latest
        container_name: "druid-middlemanager"
        hostname: middlemanager
        networks:
            - druid-net
        ports:
            - "8091:8091"
        depends_on: 
            - postgres-druid
            - zookeeper
            - minio
            - coordinator
        env_file:
            - dockerfiles/druid/druid_environment
        environment:
            - DRUID_SERVICE=middleManager
            - DRUID_JVM_ARGS=-server -Xms64m -Xmx64m -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
            - S3_ACCESS_KEY=${S3_ACCESS_KEY}
            - S3_SECRET_KEY=${S3_SECRET_KEY}
            - METADATA_STORAGE_USER=${POSTGRES_USER_NAME}
            - METADATA_STORAGE_PASSWORD=${POSTGRES_PASSWORD}


    router:
        build: dockerfiles/druid
        image: druid-router:latest
        container_name: "druid-router"
        hostname: router
        networks:
            - druid-net
        ports:
            - "8080:8080"
        depends_on: 
            - postgres-druid
            - zookeeper
            - minio
            - coordinator
            - broker  
        env_file:
            - dockerfiles/druid/druid_environment  
        environment:
            - DRUID_SERVICE=router
            - DRUID_JVM_ARGS=-server -Xms2g -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
            - S3_ACCESS_KEY=${S3_ACCESS_KEY}
            - S3_SECRET_KEY=${S3_SECRET_KEY}

networks:
  druid-net:
    driver: bridge

volumes:
    pg_data_druid:
        external: false
        name: pg_data_druid
    pg_data_dash:
        external: false
        name: pg_data_dash